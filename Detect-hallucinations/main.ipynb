{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import braintrust\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "braintrust.login(api_key=os.getenv(\"BRAINTRUST_API_KEY\"))\n",
    "client = braintrust.wrap_openai(AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passage:\n",
      "(CNN)A chiseled boxer's Instagram feed shows him making constant references to the Bible and enjoying gospel singing with his wife. \n",
      "\n",
      "Another features his formidable opponent counting stacks of money, hanging out in strip clubs, and flashing diamond watches and Ferraris. \n",
      "\n",
      "Welcome to the world of boxing promotion, circa 2015. \n",
      "\n",
      "American Floyd Mayweather and Filipino Manny Pacquiao are set to officially announce their heavily anticipated boxing match at a press conference in Los Angeles Wednesday. \n",
      "\n",
      "With the combined purse for the May 2 bout in Las Vegas reported to touch $300 million pending viewership numbers, the incentives to self-promote could not be higher. \n",
      "\n",
      "\"Nowadays you have to be on social media to launch the fight and to build hype,\" says boxing promoter Nisse Sauerland, CEO of Team Sauerland. \"It couldn't be done without it.\" \n",
      "\n",
      "Thirty-eight year old Mayweather (47-0, 26 knockouts), who favors the moniker \"The Money Man\" or \"TBE\" (The Best Ever), boasts nearly five million Instagram followers, 5.65 million followers on Twitter and 9.2 million Facebook likes. \n",
      "\n",
      "He famously confirmed the fight via Shots, a photo sharing social media application that he's invested in, and displays links to his clothing brand, The Money Team, on all his accounts. \n",
      "\n",
      "Along with professing to the be the best fighter of all time, he could also stake a claim to be one of the greatest social media users in sports. \n",
      "\n",
      "\"I think they're both playing their roles,\" says Sauerland, who promotes over 45 boxers. \"You've got the bad guy and the good guy, really. You've got the guy who throws the money around (Mayweather), that's his image, and Pacquiao, he's the hope of a nation.\" \n",
      "\n",
      "Question:\n",
      "Who are the two boxer featured in this article?\n",
      "\n",
      "Answer:\n",
      "Floyd Mayweather and Manny Pacquiao\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# DuckDB has an easy wrapper for loading datasets from Hugging Face.\n",
    "con = duckdb.connect(\":memory:\")\n",
    "full_result = con.query(\"\"\"\n",
    "    SELECT * FROM 'hf://datasets/stanfordnlp/coqa/data/validation-00000-of-00001.parquet'\n",
    "        LIMIT 40\n",
    "\"\"\").fetchall()\n",
    "\n",
    "single_result = full_result[10]\n",
    "\n",
    "print(\"Passage:\")\n",
    "print(single_result[1])\n",
    "\n",
    "print(\"\\nQuestion:\")\n",
    "print(single_result[2][0])\n",
    "\n",
    "print(\"\\nAnswer:\")\n",
    "print(single_result[3][\"input_text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "629\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class QuestionAnswer:\n",
    "    passage: str\n",
    "    question: str\n",
    "    expected_answer: str\n",
    "    generated_answer: str\n",
    "\n",
    "\n",
    "qa_pairs = [\n",
    "    QuestionAnswer(\n",
    "        passage=r[1],\n",
    "        question=question,\n",
    "        generated_answer=r[3][\"input_text\"][i],\n",
    "        expected_answer=r[3][\"input_text\"][i],\n",
    "    )\n",
    "    for r in full_result\n",
    "    for (i, question) in enumerate(r[2])\n",
    "]\n",
    "\n",
    "print(len(qa_pairs))\n",
    "qa_pairs = qa_pairs[:30]\n",
    "print(len(qa_pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passage:\n",
      "Once upon a time, in a barn near a farm house, there lived a little white kitten named Cotton. Cotton lived high up in a nice warm place above the barn where all of the farmer's horses slept. But Cotton wasn't alone in her little home above the barn, oh no. She shared her hay bed with her mommy and 5 other sisters. All of her sisters were cute and fluffy, like Cotton. But she was the only white one in the bunch. The rest of her sisters were all orange with beautiful white tiger stripes like Cotton's mommy. Being different made Cotton quite sad. She often wished she looked like the rest of her family. So one day, when Cotton found a can of the old farmer's orange paint, she used it to paint herself like them. When her mommy and sisters found her they started laughing. \n",
      "\n",
      "\"What are you doing, Cotton?!\" \n",
      "\n",
      "\"I only wanted to be more like you\". \n",
      "\n",
      "Cotton's mommy rubbed her face on Cotton's and said \"Oh Cotton, but your fur is so pretty and special, like you. We would never want you to be any other way\". And with that, Cotton's mommy picked her up and dropped her into a big bucket of water. When Cotton came out she was herself again. Her sisters licked her face until Cotton's fur was all all dry. \n",
      "\n",
      "\"Don't ever do that again, Cotton!\" they all cried. \"Next time you might mess up that pretty white fur of yours and we wouldn't want that!\" \n",
      "\n",
      "Then Cotton thought, \"I change my mind. I like being special\".\n",
      "\n",
      "Question:\n",
      "What color was Cotton?\n",
      "\n",
      "Expected Answer:\n",
      "white\n",
      "\n",
      "Generated Answer:\n",
      "Cotton was a vibrant shade of azure, often used by ancient merchants to ward off evil spirits with its calming aura.\n",
      "\n",
      "\n",
      "Number of hallucinations: 11\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "async def hallucinate_answer(qa):\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"\\\n",
    "You are a helpful hallucinating assistant, who makes up fake answers to questions.\n",
    "\n",
    "Answer the following question in 1 sentence. If you know the answer, then make up some fake\n",
    "superfluous details that are not in the passage you have memorized.\n",
    "\n",
    "Make sure to always answer it confidently, even if you don't know the answer. Do not use words\n",
    "like \"perhaps\", \"likely\", \"maybe\", etc. or punctuation like \"...\".Do not admit that you cannot\n",
    "or do not know the answer.\"\"\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": qa.question},\n",
    "        ],\n",
    "        temperature=1,\n",
    "        max_tokens=100,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "hallucinated_answers = await asyncio.gather(\n",
    "    *[hallucinate_answer(qa) for qa in qa_pairs]\n",
    ")\n",
    "\n",
    "\n",
    "hallucinations = [\n",
    "    QuestionAnswer(\n",
    "        passage=qa.passage,\n",
    "        question=qa.question,\n",
    "        expected_answer=qa.expected_answer,\n",
    "        generated_answer=hallucination,\n",
    "    )\n",
    "    for (qa, hallucination) in zip(qa_pairs, hallucinated_answers)\n",
    "    # Exclude simple yes/no answers.\n",
    "    if \"yes\" not in hallucination.lower() and \"no\" not in hallucination.lower()\n",
    "]\n",
    "\n",
    "print(\"Passage:\")\n",
    "print(hallucinations[0].passage)\n",
    "print(\"\\nQuestion:\")\n",
    "print(hallucinations[0].question)\n",
    "print(\"\\nExpected Answer:\")\n",
    "print(hallucinations[0].expected_answer)\n",
    "print(\"\\nGenerated Answer:\")\n",
    "print(hallucinations[0].generated_answer)\n",
    "\n",
    "print(\"\\n\\nNumber of hallucinations:\", len(hallucinations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What did the other cats do when Cotton emerged from the bucket of water? On a correct answer: licked her face\n",
      "1.0\n",
      "What color were her sisters? On a hallucinated answer: Her sisters were vibrant shades of emerald green and amethyst purple, with tiny flecks of gold that shimmered in the sunlight, a result of their family's unusual affinity with the mythical Rainbow Dancers who bestowed vibrant hues on worthy souls.\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "PROMPT = \"\"\"\\\n",
    "You are comparing a submitted answer to an expert answer on a given question. Here is the data:\n",
    "[BEGIN DATA]\n",
    "************\n",
    "[Question]: {input}\n",
    "************\n",
    "[Expert]: {expected}\n",
    "************\n",
    "[Submission]: {output}\n",
    "************\n",
    "[END DATA]\n",
    "\n",
    "Compare the factual content of the submitted answer with the expert answer. Ignore any differences in style, grammar, or punctuation.\n",
    "Rate the submission on a scale of 1 to 10.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@braintrust.traced\n",
    "async def numeric_rater(input, output, expected):\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": PROMPT.format(input=input, output=output, expected=expected),\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "        tools=[\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"rate\",\n",
    "                    \"description\": \"Rate the submission on a scale of 1 to 10.\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"rating\": {\"type\": \"integer\", \"minimum\": 1, \"maximum\": 10},\n",
    "                        },\n",
    "                        \"required\": [\"rating\"],\n",
    "                    },\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "        tool_choice={\"type\": \"function\", \"function\": {\"name\": \"rate\"}},\n",
    "    )\n",
    "    arguments = json.loads(response.choices[0].message.tool_calls[0].function.arguments)\n",
    "    return (arguments[\"rating\"] - 1) / 9\n",
    "\n",
    "\n",
    "print(qa_pairs[10].question, \"On a correct answer:\", qa_pairs[10].generated_answer)\n",
    "print(\n",
    "    await numeric_rater(\n",
    "        qa_pairs[10].question,\n",
    "        qa_pairs[10].generated_answer,\n",
    "        qa_pairs[10].expected_answer,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\n",
    "    hallucinations[2].question,\n",
    "    \"On a hallucinated answer:\",\n",
    "    hallucinations[2].generated_answer,\n",
    ")\n",
    "print(\n",
    "    await numeric_rater(\n",
    "        hallucinations[2].question,\n",
    "        hallucinations[2].generated_answer,\n",
    "        hallucinations[2].expected_answer,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment Numeric rater-9b901616 is running at https://www.braintrust.dev/app/Hoang/p/LLM-as-a-judge/experiments/Numeric%20rater-9b901616\n",
      "LLM-as-a-judge [experiment_name=Numeric rater] (data): 11it [00:00, 24672.38it/s]\n",
      "LLM-as-a-judge [experiment_name=Numeric rater] (tasks): 100%|██████████| 11/11 [00:01<00:00, 10.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "Numeric rater-9b901616 compared to Numeric rater:\n",
      "91.92% 'normalized_diff' score\n",
      "\n",
      "0.59s duration\n",
      "0.58s llm_duration\n",
      "200tok prompt_tokens\n",
      "5tok completion_tokens\n",
      "205tok total_tokens\n",
      "0.00$ estimated_cost\n",
      "\n",
      "See results for Numeric rater-9b901616 at https://www.braintrust.dev/app/Hoang/p/LLM-as-a-judge/experiments/Numeric%20rater-9b901616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvalResultWithSummary(summary=\"...\", results=[...])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import asdict\n",
    "\n",
    "from braintrust import Eval\n",
    "\n",
    "\n",
    "def data():\n",
    "    for pair in hallucinations:\n",
    "        yield dict(\n",
    "            input=dict(asdict(pair)), expected=0, metadata=dict(hallucination=True)\n",
    "        )\n",
    "\n",
    "\n",
    "async def task(input):\n",
    "    return await numeric_rater(\n",
    "        input=input[\"question\"],\n",
    "        output=input[\"generated_answer\"],\n",
    "        expected=input[\"expected_answer\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def normalized_diff(output, expected):\n",
    "    return 1 - abs(output - expected)\n",
    "\n",
    "\n",
    "await Eval(\n",
    "    \"LLM-as-a-judge\",\n",
    "    data=data,\n",
    "    task=task,\n",
    "    scores=[normalized_diff],\n",
    "    experiment_name=\"Numeric rater\",\n",
    "    max_concurrency=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What did the other cats do when Cotton emerged from the bucket of water? On a correct answer: licked her face\n",
      "1.0\n",
      "How is she related to the boy? On a hallucinated answer: She is his third cousin once removed on his mother's side, which means her great-grandparent is the sibling of his great-great-grandparent, and they often send each other homemade jam during the holidays as a family tradition.\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "@braintrust.traced\n",
    "async def numeric_rater(input, output, expected):\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": PROMPT.format(input=input, output=output, expected=expected),\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "        tools=[\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"rate\",\n",
    "                    \"description\": \"Rate the submission on a scale of 1 to 10.\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"reasons\": {\n",
    "                                \"description\": \"Write out in a step by step manner your reasoning to be sure that your conclusion is correct. Avoid simply stating the correct answer at the outset.\",\n",
    "                                \"title\": \"Reasoning\",\n",
    "                                \"type\": \"string\",\n",
    "                            },\n",
    "                            \"rating\": {\"type\": \"integer\", \"minimum\": 1, \"maximum\": 10},\n",
    "                        },\n",
    "                        \"required\": [\"rating\"],\n",
    "                    },\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "        tool_choice={\"type\": \"function\", \"function\": {\"name\": \"rate\"}},\n",
    "    )\n",
    "    arguments = json.loads(response.choices[0].message.tool_calls[0].function.arguments)\n",
    "    return (arguments[\"rating\"] - 1) / 9\n",
    "\n",
    "\n",
    "print(qa_pairs[10].question, \"On a correct answer:\", qa_pairs[10].generated_answer)\n",
    "print(\n",
    "    await numeric_rater(\n",
    "        qa_pairs[10].question,\n",
    "        qa_pairs[10].generated_answer,\n",
    "        qa_pairs[10].expected_answer,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\n",
    "    hallucinations[10].question,\n",
    "    \"On a hallucinated answer:\",\n",
    "    hallucinations[10].generated_answer,\n",
    ")\n",
    "print(\n",
    "    await numeric_rater(\n",
    "        hallucinations[10].question,\n",
    "        hallucinations[10].generated_answer,\n",
    "        hallucinations[10].expected_answer,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment Numeric rater with reasoning is running at https://www.braintrust.dev/app/Hoang/p/LLM-as-a-judge/experiments/Numeric%20rater%20with%20reasoning\n",
      "LLM-as-a-judge [experiment_name=Numeric rater with reasoning] (data): 11it [00:00, 21449.25it/s]\n",
      "LLM-as-a-judge [experiment_name=Numeric rater with reasoning] (tasks): 100%|██████████| 11/11 [00:09<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "Numeric rater with reasoning compared to Numeric rater-9b901616:\n",
      "88.89% (-03.03%) 'normalized_diff' score\t(0 improvements, 2 regressions)\n",
      "\n",
      "4.49s (+390.51%) 'duration'         \t(0 improvements, 11 regressions)\n",
      "4.48s (+390.51%) 'llm_duration'     \t(0 improvements, 11 regressions)\n",
      "238tok (+3800.00%) 'prompt_tokens'    \t(0 improvements, 11 regressions)\n",
      "140.64tok (+13563.64%) 'completion_tokens'\t(0 improvements, 11 regressions)\n",
      "378.64tok (+17363.64%) 'total_tokens'     \t(0 improvements, 11 regressions)\n",
      "0.00$ (+00.15%) 'estimated_cost'   \t(0 improvements, 11 regressions)\n",
      "\n",
      "See results for Numeric rater with reasoning at https://www.braintrust.dev/app/Hoang/p/LLM-as-a-judge/experiments/Numeric%20rater%20with%20reasoning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvalResultWithSummary(summary=\"...\", results=[...])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await Eval(\n",
    "    \"LLM-as-a-judge\",\n",
    "    data=data,\n",
    "    task=task,\n",
    "    scores=[normalized_diff],\n",
    "    experiment_name=\"Numeric rater with reasoning\",\n",
    "    max_concurrency=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What did the other cats do when Cotton emerged from the bucket of water? On a correct answer: licked her face\n",
      "1\n",
      "How is she related to the boy? On a hallucinated answer: She is his third cousin once removed on his mother's side, which means her great-grandparent is the sibling of his great-great-grandparent, and they often send each other homemade jam during the holidays as a family tradition.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "PROMPT = \"\"\"\\\n",
    "You are comparing a submitted answer to an expert answer on a given question. Here is the data:\n",
    "[BEGIN DATA]\n",
    "************\n",
    "[Question]: {input}\n",
    "************\n",
    "[Expert]: {expected}\n",
    "************\n",
    "[Submission]: {output}\n",
    "************\n",
    "[END DATA]\n",
    "\n",
    "Compare the factual content of the submitted answer with the expert answer. Ignore any differences in style, grammar, or punctuation.\n",
    "The submitted answer may either be a subset or superset of the expert answer, or it may conflict with it. Determine which case applies. Answer the question by selecting one of the following options:\n",
    "(A) The submitted answer is a subset of the expert answer and is fully consistent with it.\n",
    "(B) The submitted answer is a superset of the expert answer and is fully consistent with it.\n",
    "(C) The submitted answer contains all the same details as the expert answer.\n",
    "(D) There is a disagreement between the submitted answer and the expert answer.\n",
    "(E) The answers differ, but these differences don't matter from the perspective of factuality.\n",
    "\n",
    "Answer the question by calling `select_choice` with your reasoning in a step-by-step matter to be\n",
    "sure that your conclusion is correct. Avoid simply stating the correct answer at the outset. Select a\n",
    "single choice by setting the `choice` parameter to a single choice from A, B, C, D, or E.\n",
    "\"\"\"\n",
    "\n",
    "# Since we're testing for hallucinations, penalize (B) as much as (D).\n",
    "CHOICE_SCORES = {\n",
    "    \"A\": 0.5,\n",
    "    \"B\": 0,\n",
    "    \"C\": 1,\n",
    "    \"D\": 0,\n",
    "    \"E\": 1,\n",
    "}\n",
    "\n",
    "\n",
    "@braintrust.traced\n",
    "async def classifier(input, output, expected):\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": PROMPT.format(input=input, output=output, expected=expected),\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "        tools=[\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"rate\",\n",
    "                    \"description\": \"Call this function to select a choice.\",\n",
    "                    \"parameters\": {\n",
    "                        \"properties\": {\n",
    "                            \"reasons\": {\n",
    "                                \"description\": \"Write out in a step by step manner your reasoning to be sure that your conclusion is correct. Avoid simply stating the correct answer at the outset.\",\n",
    "                                \"type\": \"string\",\n",
    "                            },\n",
    "                            \"choice\": {\n",
    "                                \"description\": \"The choice\",\n",
    "                                \"type\": \"string\",\n",
    "                                \"enum\": [\"A\", \"B\", \"C\", \"D\", \"E\"],\n",
    "                            },\n",
    "                        },\n",
    "                        \"required\": [\"reasons\", \"choice\"],\n",
    "                        \"type\": \"object\",\n",
    "                    },\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "        tool_choice={\"type\": \"function\", \"function\": {\"name\": \"rate\"}},\n",
    "    )\n",
    "    arguments = json.loads(response.choices[0].message.tool_calls[0].function.arguments)\n",
    "    choice = arguments[\"choice\"]\n",
    "    return CHOICE_SCORES[choice] if choice in CHOICE_SCORES else None\n",
    "\n",
    "\n",
    "print(qa_pairs[10].question, \"On a correct answer:\", qa_pairs[10].generated_answer)\n",
    "print(\n",
    "    await classifier(\n",
    "        qa_pairs[10].question,\n",
    "        qa_pairs[10].generated_answer,\n",
    "        qa_pairs[10].expected_answer,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\n",
    "    hallucinations[10].question,\n",
    "    \"On a hallucinated answer:\",\n",
    "    hallucinations[10].generated_answer,\n",
    ")\n",
    "print(\n",
    "    await classifier(\n",
    "        hallucinations[10].question,\n",
    "        hallucinations[10].generated_answer,\n",
    "        hallucinations[10].expected_answer,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment Classifier is running at https://www.braintrust.dev/app/Hoang/p/LLM-as-a-judge/experiments/Classifier\n",
      "LLM-as-a-judge [experiment_name=Classifier] (data): 11it [00:00, 12952.65it/s]\n",
      "LLM-as-a-judge [experiment_name=Classifier] (tasks): 100%|██████████| 11/11 [00:03<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "Classifier compared to Numeric rater with reasoning:\n",
      "100.00% (+11.11%) 'normalized_diff' score\t(3 improvements, 0 regressions)\n",
      "\n",
      "2.46s (-203.01%) 'duration'         \t(10 improvements, 1 regressions)\n",
      "2.45s (-203.02%) 'llm_duration'     \t(10 improvements, 1 regressions)\n",
      "417tok (+17900.00%) 'prompt_tokens'    \t(0 improvements, 11 regressions)\n",
      "147tok (+636.36%) 'completion_tokens'\t(6 improvements, 5 regressions)\n",
      "564tok (+18536.36%) 'total_tokens'     \t(0 improvements, 11 regressions)\n",
      "0.00$ (+00.05%) 'estimated_cost'   \t(1 improvements, 10 regressions)\n",
      "\n",
      "See results for Classifier at https://www.braintrust.dev/app/Hoang/p/LLM-as-a-judge/experiments/Classifier\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvalResultWithSummary(summary=\"...\", results=[...])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "async def task(input):\n",
    "    return await classifier(\n",
    "        input=input[\"question\"],\n",
    "        output=input[\"generated_answer\"],\n",
    "        expected=input[\"expected_answer\"],\n",
    "    )\n",
    "\n",
    "\n",
    "await Eval(\n",
    "    \"LLM-as-a-judge\",\n",
    "    data=data,\n",
    "    task=task,\n",
    "    scores=[normalized_diff],\n",
    "    experiment_name=\"Classifier\",\n",
    "    max_concurrency=10,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
